{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekQPgGVwQb1a"
   },
   "source": [
    "# Exercise #3 - Human in the Loop\n",
    "\n",
    "In the previous exercise, you called an Activity to get the topic of a sentence using an LLM and used an LLM to create an image of that topic, finally adding it to your research report.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Review a modified version of the previous exercise and investigate the results in the Web UI\n",
    "2. Add a Signal to the exercise to provide the filename you wish to save the research report as\n",
    "3. Add a Query to the exercise to extract the character length of the research request\n",
    "\n",
    "Fill in the TODO instructions. Go to the `solution` directory if you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_uA13X5SYWe"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Before doing the exercise, you need to:\n",
    "\n",
    "- Install necessary dependencies\n",
    "- Create your `.env` file and supply your API key\n",
    "- Load the environment variables\n",
    "- Download and start a local Temporal Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17937,
     "status": "ok",
     "timestamp": 1756905767186,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "m8UH2Il_Sp50",
    "outputId": "ab260520-5ad6-452e-fc53-247922d08c14"
   },
   "outputs": [],
   "source": [
    "%pip install --quiet temporalio litellm reportlab python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXp3VIAdZuhO"
   },
   "source": [
    "### Create a `.env` File\n",
    "\n",
    "Next you'll create a `.env` file to store your API keys.\n",
    "In the file browser on the left, create a new file and name it `.env`.\n",
    "\n",
    "**Note**: It may disappear as soon as you create it. This is because Google Collab hides hidden files (files that start with a `.`) by default.\n",
    "To make this file appear, click the icon that is a crossed out eye and hidden files will appear.\n",
    "\n",
    "Then double click on the `.env` file and add the following line with your API key.\n",
    "\n",
    "```\n",
    "LLM_API_KEY = YOUR_API_KEY\n",
    "LLM_MODEL = \"openai/gpt-4o\"\n",
    "```\n",
    "\n",
    "By default this notebook uses OpenAI's GPT-4o.\n",
    "If you want to use a different LLM provider, look up the appropriate model name [in their documentation](https://docs.litellm.ai/docs/providers) and change the `LLM_MODEL` field and provide your API key.\n",
    "\n",
    "**To perform image generation, you will need an OpenAI key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSl-K8ATXLJ3"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "# Now open the file and replace YOUR_API_KEY with your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PU75hvN-Qed"
   },
   "source": [
    "### Add Your LLM API Key **Before** Running the Following Code Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1756905796783,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "fy4s0KTRdWxL",
    "outputId": "755db2ac-5430-4439-db32-f37ee8916fed"
   },
   "outputs": [],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and confirm it is available without printing the value\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "if not LLM_API_KEY:\n",
    "    raise RuntimeError(\"LLM_API_KEY environment variable is not set. Please update your .env file.\")\n",
    "print(\"LLM API key loaded from environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90AW0nTB0P2V"
   },
   "source": [
    "### Setting Up the Temporal Service\n",
    "\n",
    "Run the following blocks to setup & enable a local Temporal Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLHoJlR4R8Au"
   },
   "outputs": [],
   "source": [
    "# allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2929,
     "status": "ok",
     "timestamp": 1756905801746,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "scrsqIOy0a_8",
    "outputId": "4e3190b2-02aa-4964-b9c9-29935cedca87"
   },
   "outputs": [],
   "source": [
    "# Download the Temporal CLI.\n",
    "\n",
    "!curl -sSf https://temporal.download/cli.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Sure Your Temporal Web UI is Running\n",
    "\n",
    "1. You should have the Temporal Server running in your terminal (run `temporal server start-dev` if not).\n",
    "2. Then in your `Ports` tab on the bottom of this screen, find `8233` and click on the Globe icon to open the Temporal Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw22wN9wX1Ok"
   },
   "source": [
    "### Part 1 - Run the below code blocks to load them into the proram\n",
    "\n",
    "Review this code and run it to understand what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSWFzYyb2UBJ"
   },
   "source": [
    "### Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMHdjdjL2FlT"
   },
   "outputs": [],
   "source": [
    "# Run this codeblock\n",
    "from dataclasses import dataclass\n",
    "from enum import StrEnum\n",
    "\n",
    "class UserDecision(StrEnum):\n",
    "    KEEP = \"KEEP\"\n",
    "    EDIT = \"EDIT\"\n",
    "    WAIT = \"WAIT\"\n",
    "\n",
    "@dataclass\n",
    "class LLMCallInput:\n",
    "  prompt: str\n",
    "  llm_api_key: str\n",
    "  llm_model: str\n",
    "\n",
    "@dataclass\n",
    "class PDFGenerationInput:\n",
    "  content: str\n",
    "  image_url: str | None = None\n",
    "  filename: str = \"research_pdf\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportInput:\n",
    "    prompt: str\n",
    "    llm_api_key: str\n",
    "    llm_research_model: str = \"openai/gpt-4o\"\n",
    "    llm_image_model: str = \"dall-e-3\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportOutput:\n",
    "    result: str\n",
    "\n",
    "@dataclass\n",
    "class GenerateImageInput:\n",
    "    topic: str\n",
    "    llm_api_key: str\n",
    "    llm_model: str = \"dall-e-3\"\n",
    "\n",
    "@dataclass\n",
    "class UserDecisionSignal:\n",
    "    decision: UserDecision\n",
    "    additional_prompt: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys60mB8o2Vib"
   },
   "source": [
    "### Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FEDm6EiYKB2"
   },
   "outputs": [],
   "source": [
    "# Run this codeblock\n",
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "from litellm import completion, ModelResponse, image_generation\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "from temporalio import activity\n",
    "\n",
    "@activity.defn\n",
    "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
    "    response = completion(\n",
    "      model=input.llm_model,\n",
    "      api_key=input.llm_api_key,\n",
    "      messages=[{ \"content\": input.prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "@activity.defn\n",
    "def generate_ai_image(input: GenerateImageInput) -> ModelResponse:\n",
    "\n",
    "    image_prompt = f\"A cute, natural image of {input.topic}.\"\n",
    "\n",
    "    response = image_generation(\n",
    "        prompt=image_prompt,\n",
    "        model=input.llm_model,\n",
    "        api_key=input.llm_api_key\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "@activity.defn\n",
    "def create_pdf_activity(input: PDFGenerationInput) -> str:\n",
    "    doc = SimpleDocTemplate(f\"{input.filename}.pdf\", pagesize=letter)\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "\n",
    "    if input.image_url is not None:\n",
    "      img_response = requests.get(input.image_url)\n",
    "      img_buffer = BytesIO(img_response.content)\n",
    "      img = RLImage(img_buffer, width=5*inch, height=5*inch)\n",
    "      story.append(img)\n",
    "      story.append(Spacer(1, 20))\n",
    "\n",
    "    paragraphs = input.content.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "            p = Paragraph(para.strip(), styles['Normal'])\n",
    "            story.append(p)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "    return input.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this codeblock\n",
    "@dataclass\n",
    "class FilenameSave:\n",
    "  filename: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BE_tNaFU_dP"
   },
   "source": [
    "### Part 2 - Implementing a Signal\n",
    "\n",
    "Next you'll implement a Signal yourself. This Signal will act as a prompt that provides the filename to save the file as. However, if the user doesn't provide a response within twenty seconds, a default will be used instead.\n",
    "\n",
    "To do this, we be using the [wait_condition](https://python.temporal.io/temporalio.workflow.html#wait_condition) method.\n",
    "\n",
    "1. Add a new attribute, called `filename` and default it to a string set at `research_report`.\n",
    "2. Decorate the `filename_signal` Signal with `@workflow.signal`.\n",
    "3. In the `wait_condition`, in the first parameter, set the condition that the `self._filename` attribute does not equal to \"research_report\"\n",
    "4. In the `wait_condition`, in the second parameter, set the `wait_condition` to take in a timeout parameter of 20 seconds.\n",
    "5. Run the code block to load it into the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-vJaE232x1h"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str | None = None\n",
    "        self._filename: str = \"research_report\"\n",
    "\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.signal\n",
    "    async def filename_signal(self, input: FilenameSave) -> None:\n",
    "        self._filename = input.filename\n",
    "\n",
    "    @workflow.query\n",
    "    def get_research_result(self) -> str | None:\n",
    "        return self._research_result\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            # Store the research result for queries\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            print(\"Research complete!\")\n",
    "\n",
    "            print(\"Waiting for user decision. Send signal with 'keep' to create PDF or 'edit' to modify prompt.\")\n",
    "\n",
    "            await workflow.wait_condition(\n",
    "                lambda: self._user_decision.decision != UserDecision.WAIT\n",
    "            )\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                print(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                print(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    print(\n",
    "                        f\"Regenerating research with updated prompt: {self._current_prompt}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"No additional instructions provided. Regenerating with original prompt.\"\n",
    "                    )\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        subject_prompt = f\"What is the main topic of this sentence? Respond with only the topic in a single word or short phrase if multiple topics are detected. This response will be used for generating an AI image. No explanation. The sentence is: {self._current_prompt}\"\n",
    "        subject_input = LLMCallInput(prompt=subject_prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        topic_call = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            subject_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        topic = topic_call[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Used the new GenerateImageInput dataclass to create the input object for the Activity\n",
    "        image_input = GenerateImageInput(topic=topic, llm_api_key=LLM_API_KEY)\n",
    "\n",
    "        # Called the new generate_ai_image Activity, passing in the image_input parameter made above\n",
    "        ai_image = await workflow.execute_activity(\n",
    "            generate_ai_image,\n",
    "            image_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        # Exctract the image_url form the Activity call\n",
    "        image_url = ai_image[\"data\"][0][\"url\"]\n",
    "\n",
    "        try:\n",
    "          await workflow.wait_condition(\n",
    "              lambda: self._filename != \"research_report\",\n",
    "              timeout=timedelta(seconds=20)\n",
    "          )\n",
    "        except asyncio.TimeoutError:\n",
    "          print(\"20 seconds have passed. The program will continue and your file will automatically be named 'research_paper.pdf'.\")\n",
    "\n",
    "        # Add the image_url parameter to the PDF Generation so the image is included\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"], image_url=image_url, filename=self._filename)\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            create_pdf_activity,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Prepare your Worker\n",
    "\n",
    "1. Add your `generate_ai_image` Activity to your list of Activities registered on the Worker\n",
    "2. Run the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNZZ71MHBwqp"
   },
   "outputs": [],
   "source": [
    "from temporalio.client import Client\n",
    "from temporalio.worker import Worker\n",
    "import concurrent.futures\n",
    "\n",
    "async def run_worker() -> None:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "    # Run the Worker\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
    "        worker = Worker(\n",
    "            client,\n",
    "            task_queue=\"research\",\n",
    "            workflows=[GenerateReportWorkflow],\n",
    "            activities=[llm_call, create_pdf_activity, generate_ai_image],\n",
    "            activity_executor=activity_executor\n",
    "        )\n",
    "\n",
    "        print(f\"Starting the worker....\")\n",
    "        await worker.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUpctA0H56gl"
   },
   "outputs": [],
   "source": [
    "# Start a new worker\n",
    "\n",
    "# If you didn't kill the previous worker, uncomment this and run it first\n",
    "# worker.cancel()\n",
    "import asyncio\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your Workflow\n",
    "# Run this code block\n",
    "import uuid\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Sending a Signal with the Client\n",
    "\n",
    "1. After we get the filename from the input, send the `filename_signal` to the Workflow handle, passing in `FilenameSave(filename=filename))`. Don't forget to use await.\n",
    "2. Run the code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(workflow_id)\n",
    "    loop = asyncio.get_running_loop()\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\n",
    "            \"Calling LLM! See the response in your Web UI in the output of the `llm_call` Activity. Would you like to keep or edit it?\"\n",
    "        )\n",
    "        print(\"1. Type 'keep' to approve the output and create PDF\")\n",
    "        print(\"2. Type 'edit' to modify the output\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # When running input in async code, run in an executor to not block the event loop\n",
    "        decision = await loop.run_in_executor(None, input, \"Your decision (keep/edit): \")\n",
    "        decision = decision.strip().lower()\n",
    "\n",
    "        if decision in {\"keep\", \"1\"}:\n",
    "            signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n",
    "            await handle.signal(\"user_decision_signal\", signal_data) # Send our Keep Signal to our Workflow Execution we have a handle on\n",
    "            print(\"Signal sent to keep output and create PDF\")\n",
    "            break\n",
    "        if decision in {\"edit\", \"2\"}:\n",
    "            additional_prompt_input = input(\"Enter additional instructions to edit the output (optional): \").strip()\n",
    "            additional_prompt = additional_prompt_input if additional_prompt_input else \"\"\n",
    "            signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n",
    "            await handle.signal(\"user_decision_signal\", signal_data)\n",
    "            print(\"Signal sent to regenerate research\")\n",
    "\n",
    "        else:\n",
    "            print(\"Please enter either 'keep', 'edit'\")\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"What do you want to name the file? After 20 seconds, the program will continue and your file will automatically be named 'research_paper.pdf'.\")\n",
    "    filename = await loop.run_in_executor(None, input, \"Enter the filename: \")\n",
    "    await handle.signal(\"filename_signal\", FilenameSave(filename=filename))\n",
    "    end_time = datetime.datetime.now()\n",
    "    if end_time - start_time > datetime.timedelta(seconds=20):\n",
    "        print(\"20 seconds have passed. The program will continue and your file will automatically be named 'research_paper.pdf'.\")\n",
    "    else:\n",
    "        print(f\"Your file will be saved as {filename}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaU2t6GERaN3"
   },
   "source": [
    "### Test Your Signal\n",
    "\n",
    "Run the code, and provide a filename for it to be saved as within the 20 seconds timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18877,
     "status": "ok",
     "timestamp": 1756905912307,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "GXJjTpMo5mHe",
    "outputId": "a4e272ce-3776-4196-9482-6562c60deda3"
   },
   "outputs": [],
   "source": [
    "signal_task = asyncio.create_task(send_user_decision_signal(client, handle.id))\n",
    "\n",
    "try:\n",
    "    result = await handle.result()\n",
    "    signal_task.cancel()\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    signal_task.cancel()\n",
    "    print(f\"Workflow failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your file explorer to see your new file\n",
    "\n",
    "It should be named the file you gave it. To view it, right click, click Download. Open it from your downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4iN3KNdRkPG"
   },
   "source": [
    "### Test Your Signal - Timeout Path\n",
    "\n",
    "Let's test what happens when you don't give your file a name in 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill any previous workers that may still be running\n",
    "x = worker.cancel()\n",
    "\n",
    "# Start a new worker\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your Workflow\n",
    "# Run this code block\n",
    "import uuid\n",
    "from temporalio.client import Client\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send your Signal. Run this code block.\n",
    "# Enter \"Keep\" to generate the PDF\n",
    "# After 20 seconds is up, you'll see \"research_result.pdf\" saved in your file explorer since you did not give it a filename\n",
    "# Click the Escape button\n",
    "import asyncio\n",
    "\n",
    "signal_task = asyncio.create_task(send_user_decision_signal(client, handle.id))\n",
    "\n",
    "try:\n",
    "    result = await handle.result()\n",
    "    signal_task.cancel()\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    signal_task.cancel()\n",
    "    print(f\"Workflow failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaIYuART6KKY"
   },
   "source": [
    "### Watch the Execuction in the Web UI\n",
    "\n",
    "Open the Web UI and compare the two different Workflow Executions.\n",
    "\n",
    "- What was different in the Event History/Timeline view between the two?\n",
    "- How did the Activity know what to name the file? Can you find how this data is relayed in the Event History?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqb7u6GPKhAC"
   },
   "source": [
    "### Part 5 - Adding Queries\n",
    "\n",
    "We'll now add queries! You've already seen an example of looking at the research content in the last notebook. Let's now add in a query to see the character count of your generated research.\n",
    " \n",
    "1. We'll first add in a new attribute, called `character_count` that is set to an integer. It should default to 0.\n",
    "2. Handle a `get_research_stats` Query by anotating it with `@workflow.query`. Have it return its attribute `character_count`.\n",
    "3. After the line where your Workflow sets the `research_result` variable, set your `character_count` attribute to `len(research_facts[\"choices\"][0][\"message\"][\"content\"])`.\n",
    "4. Run the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiBHlQdEKkV9"
   },
   "outputs": [],
   "source": [
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        self._character_count: int = 0\n",
    "\n",
    "    @workflow.query\n",
    "    def get_research_stats(self) -> int:\n",
    "        return self._character_count\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        self._character_count = len(research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf_activity,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ednmmCmzmPDZ"
   },
   "outputs": [],
   "source": [
    "# Kill any previous workers that may still be running\n",
    "x = worker.cancel()\n",
    "\n",
    "# Start a new worker\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6 - Sending a Query with the Client\n",
    "\n",
    "1. In the `query_research_stats` function, in the `query` function, pass in `GenerateReportWorkflow.get_research_stats`.\n",
    "2. In the `print(f\"Research character count: {}\")` statement, print out your `stats` variable.\n",
    "3. Run the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6116,
     "status": "ok",
     "timestamp": 1756906633025,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "rRKGBkkqmp63",
    "outputId": "60fa90cd-ad17-4a0e-8d5a-d318ca355afb"
   },
   "outputs": [],
   "source": [
    "from temporalio.client import Client\n",
    "\n",
    "async def query_research_stats(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(workflow_id)\n",
    "    try:\n",
    "        stats = await handle.query(GenerateReportWorkflow.get_research_stats)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Research character count: {stats}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Research stats query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your Workflow\n",
    "# Run this code block\n",
    "from temporalio.client import Client\n",
    "import uuid\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExpoxUE3n1te"
   },
   "outputs": [],
   "source": [
    "# Run this to send your Query\n",
    "import asyncio\n",
    "\n",
    "query_task = asyncio.create_task(query_research_stats(client, handle.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill any worker to prepare for the next chapter.\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "  print(\"Worker killed\")\n",
    "else:\n",
    "  print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "This workshop introduced you to the **interacting with workflows** in Temporal. Further your learning with these resources:\n",
    "\n",
    "- Our [free course](https://learn.temporal.io/courses/interacting_with_workflows/python/) on Signals and Queries in Temporal and other ways to interact with Workflows\n",
    "- A [Python tutorial](https://learn.temporal.io/tutorials/python/build-an-email-drip-campaign/) to practice handling Signals and Queries\n",
    "- Documentation on [Updates](https://docs.temporal.io/develop/python/message-passing#updates), a feature which combine both Signals and Queries"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python (edu-ai-workshop-mcp)",
   "language": "python",
   "name": "edu-ai-workshop-mcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
