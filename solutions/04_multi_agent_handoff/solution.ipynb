{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16aec450",
   "metadata": {},
   "source": [
    "# Exercise 4: Multi-Agent Handoff â€” Solution\n",
    "\n",
    "Complete implementation of the multi-agent handoff workflow with Temporal orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d7668",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Confirm the Temporal dev server is running and `.env` is configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9aba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet temporalio openai rich\n",
    "\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import uuid\n",
    "from datetime import timedelta\n",
    "from openai import OpenAI\n",
    "from rich.console import Console\n",
    "from temporalio import activity, workflow\n",
    "from temporalio.client import Client\n",
    "from temporalio.common import RetryPolicy\n",
    "from temporalio.worker import Worker, UnsandboxedWorkflowRunner\n",
    "\n",
    "# Shared console instance for consistent workshop output\n",
    "console = Console()\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "async def get_weather(location: str) -> str:\n",
    "    \"\"\"Return mock weather details for the requested location.\"\"\"\n",
    "    # Provide canned weather data for known locations\n",
    "    weather_data = {\n",
    "        \"San Francisco\": \"sunny, 72Â°F\",\n",
    "        \"New York\": \"cloudy, 65Â°F\",\n",
    "        \"London\": \"rainy, 58Â°F\",\n",
    "        \"Tokyo\": \"clear, 70Â°F\",\n",
    "    }\n",
    "    # Select the known weather or fall back to a default description\n",
    "    weather = weather_data.get(location, \"partly cloudy, 68Â°F\")\n",
    "    # Return a formatted weather string consumed by the LLM\n",
    "    return f\"The weather in {location} is {weather}\"\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "async def get_time(location: str) -> str:\n",
    "    \"\"\"Return mock local time for the requested location.\"\"\"\n",
    "    # Provide canned time data for known locations\n",
    "    time_data = {\n",
    "        \"San Francisco\": \"10:45 AM PST\",\n",
    "        \"New York\": \"1:45 PM EST\",\n",
    "        \"London\": \"6:45 PM GMT\",\n",
    "        \"Tokyo\": \"3:45 AM JST\",\n",
    "    }\n",
    "    # Select the known time or fall back to a neutral value\n",
    "    current_time = time_data.get(location, \"12:00 PM\")\n",
    "    # Return a formatted time string consumed by the LLM\n",
    "    return f\"The current time in {location} is {current_time}\"\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "async def triage_query(query: str) -> str:\n",
    "    \"\"\"Classify the incoming query to the appropriate specialist agent.\"\"\"\n",
    "    activity.logger.info(\"ðŸ” Triaging query: %s\", query)\n",
    "    # Instantiate the OpenAI client using the configured API key\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    # Request the classifier model to pick the best agent route\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a query classifier. Classify queries into one of: \"\n",
    "                    \"'weather_agent', 'time_agent', or 'general_agent'. \"\n",
    "                    \"Respond with ONLY the agent name, nothing else.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Read the classifier decision from the model response\n",
    "    agent_name = response.choices[0].message.content.strip()\n",
    "    activity.logger.info(\"   â†’ Routed to: %s\", agent_name)\n",
    "    # Return the target agent name to the workflow\n",
    "    return agent_name\n",
    "\n",
    "\n",
    "def _safe_parse_arguments(arg_string: str) -> dict:\n",
    "    # Attempt to parse tool call arguments as JSON\n",
    "    try:\n",
    "        return json.loads(arg_string)\n",
    "    except json.JSONDecodeError:\n",
    "        # Default to empty args if parsing fails so the agent can continue\n",
    "        activity.logger.warning(\"Failed to parse tool arguments, raw input: %s\", arg_string)\n",
    "        return {}\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "async def weather_agent(query: str, context: dict) -> str:\n",
    "    \"\"\"Handle weather-related queries with tool support.\"\"\"\n",
    "    activity.logger.info(\"ðŸŒ¤ï¸  Weather agent handling query\")\n",
    "    # Instantiate the OpenAI client for the weather specialist\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    # Describe the supported weather tool for the LLM\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current weather for a location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Seed the conversation with system and user messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a weather specialist assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    # Ask the model to respond and potentially call the weather tool\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # Extract the first message object from the model output\n",
    "    response_message = response.choices[0].message\n",
    "    # Capture any requested tool calls from the response\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    # Handle tool calls when the model asked for external data\n",
    "    if tool_calls:\n",
    "        # Include the model response before executing tools\n",
    "        messages.append(response_message)\n",
    "        # Iterate through each tool call to service the request\n",
    "        for tool_call in tool_calls:\n",
    "            # Parse the tool arguments safely\n",
    "            payload = _safe_parse_arguments(tool_call.function.arguments)\n",
    "            # Only invoke the tool when a location argument is present\n",
    "            if tool_call.function.name == \"get_weather\" and payload.get(\"location\"):\n",
    "                # Await the async weather activity to get mock data\n",
    "                tool_result = await get_weather(**payload)\n",
    "                # Attach the tool response so the LLM can use it\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": tool_call.function.name,\n",
    "                        \"content\": tool_result,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Ask the model to produce the final natural language answer\n",
    "        follow_up = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        # Use the follow-up content as the final agent reply\n",
    "        final_message = follow_up.choices[0].message.content\n",
    "    else:\n",
    "        # When no tool was called, return the original model content\n",
    "        final_message = response_message.content\n",
    "\n",
    "    # Provide the resolved answer back to the workflow\n",
    "    activity.logger.info(\"âœ… Weather agent completed\")\n",
    "    return final_message\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "async def time_agent(query: str, context: dict) -> str:\n",
    "    \"\"\"Handle time-related queries with tool support.\"\"\"\n",
    "    activity.logger.info(\"ðŸ• Time agent handling query\")\n",
    "    # Instantiate the OpenAI client for the time specialist\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    # Describe the supported time tool for the LLM\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_time\",\n",
    "                \"description\": \"Get current time for a location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Seed the conversation with system and user messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a time specialist assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    # Ask the model to respond and potentially call the time tool\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # Extract the first message object from the model output\n",
    "    response_message = response.choices[0].message\n",
    "    # Capture any requested tool calls from the response\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    # Handle tool calls when the model asked for external data\n",
    "    if tool_calls:\n",
    "        # Include the model response before executing tools\n",
    "        messages.append(response_message)\n",
    "        # Iterate through each tool call to service the request\n",
    "        for tool_call in tool_calls:\n",
    "            # Parse the tool arguments safely\n",
    "            payload = _safe_parse_arguments(tool_call.function.arguments)\n",
    "            # Only invoke the tool when a location argument is present\n",
    "            if tool_call.function.name == \"get_time\" and payload.get(\"location\"):\n",
    "                # Await the async time activity to get mock data\n",
    "                tool_result = await get_time(**payload)\n",
    "                # Attach the tool response so the LLM can use it\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": tool_call.function.name,\n",
    "                        \"content\": tool_result,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Ask the model to produce the final natural language answer\n",
    "        follow_up = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        # Use the follow-up content as the final agent reply\n",
    "        final_message = follow_up.choices[0].message.content\n",
    "    else:\n",
    "        # When no tool was called, return the original model content\n",
    "        final_message = response_message.content\n",
    "\n",
    "    # Provide the resolved answer back to the workflow\n",
    "    activity.logger.info(\"âœ… Time agent completed\")\n",
    "    return final_message\n",
    "\n",
    "\n",
    "@workflow.defn\n",
    "class MultiAgentWorkflow:\n",
    "    \"\"\"Workflow orchestrating triage and specialist agent hand-offs.\"\"\"\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, query: str, trace_id: str) -> str:\n",
    "        # Note that the workflow is starting execution\n",
    "        workflow.logger.info(\"ðŸš€ Multi-agent workflow started\")\n",
    "        # Record the incoming end-user query for traceability\n",
    "        workflow.logger.info(\"   Query: %s\", query)\n",
    "        # Record the trace identifier for cross-system debugging\n",
    "        workflow.logger.info(\"   Trace ID: %s\", trace_id)\n",
    "\n",
    "        # Track minimal conversation context for future enhancements\n",
    "        context = {\"history\": [], \"trace_id\": trace_id}\n",
    "\n",
    "        # Ask the triage activity to determine the best agent for the query\n",
    "        agent_to_use = await workflow.execute_activity(\n",
    "            triage_query,\n",
    "            args=[query],\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "            retry_policy=RetryPolicy(\n",
    "                maximum_attempts=3,\n",
    "                initial_interval=timedelta(seconds=1),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Log which agent will receive the request next\n",
    "        workflow.logger.info(\"   Routing to: %s\", agent_to_use)\n",
    "\n",
    "        # Invoke the appropriate specialist based on the triage decision\n",
    "        if agent_to_use == \"weather_agent\":\n",
    "            # Execute the weather specialist with retry safety\n",
    "            result = await workflow.execute_activity(\n",
    "                weather_agent,\n",
    "                args=[query, context],\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "                retry_policy=RetryPolicy(maximum_attempts=3),\n",
    "            )\n",
    "        elif agent_to_use == \"time_agent\":\n",
    "            # Execute the time specialist with retry safety\n",
    "            result = await workflow.execute_activity(\n",
    "                time_agent,\n",
    "                args=[query, context],\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "                retry_policy=RetryPolicy(maximum_attempts=3),\n",
    "            )\n",
    "        else:\n",
    "            # Provide a placeholder when no specific specialist exists\n",
    "            result = \"Query routed to general agent (not implemented in this exercise)\"\n",
    "\n",
    "        # Confirm completion before returning to the caller\n",
    "        workflow.logger.info(\"âœ… Multi-agent workflow completed\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699d15c",
   "metadata": {},
   "source": [
    "## Run the Solution\n",
    "\n",
    "Execute the solution workflow to observe agent handoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05a1c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸš€ Exercise </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">: Multi-Agent Handoff â€” Solution</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mðŸš€ Exercise \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;36m: Multi-Agent Handoff â€” Solution\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Trace ID:</span> <span style=\"color: #ffff00; text-decoration-color: #ffff00\">bf7e4a3f-1c60-4d91-9540-8a486796a9ab</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mTrace ID:\u001b[0m \u001b[93mbf7e4a3f-1c60-4d91-9540-8a486796a9ab\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Query:</span> What's the weather like in London and what time is it there?\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mQuery:\u001b[0m What's the weather like in London and what time is it there?\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ… Final Response:</span>\n",
       "Query routed to general agent <span style=\"font-weight: bold\">(</span>not implemented in this exercise<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mâœ… Final Response:\u001b[0m\n",
       "Query routed to general agent \u001b[1m(\u001b[0mnot implemented in this exercise\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">View in Temporal UI:</span> \n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8233/namespaces/default/workflows/multi-agent-bf7e4a3f-1c60-4d91-9540-8a486796a9ab</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mView in Temporal UI:\u001b[0m \n",
       "\u001b[4;94mhttp://localhost:8233/namespaces/default/workflows/multi-agent-bf7e4a3f-1c60-4d91-9540-8a486796a9ab\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Trace ID:</span> <span style=\"color: #ffff00; text-decoration-color: #ffff00\">bf7e4a3f-1c60-4d91-9540-8a486796a9ab</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mTrace ID:\u001b[0m \u001b[93mbf7e4a3f-1c60-4d91-9540-8a486796a9ab\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the multi-agent handoff solution\n",
    "async def run_solution() -> None:\n",
    "    # Introduce the demo execution in the notebook\n",
    "    console.print(\"\\n[bold cyan]ðŸš€ Exercise 4: Multi-Agent Handoff â€” Solution[/bold cyan]\\n\")\n",
    "\n",
    "    # Create a unique identifier to correlate workflow and traces\n",
    "    trace_id = str(uuid.uuid4())\n",
    "    # Display the trace identifier for manual debugging\n",
    "    console.print(f\"[yellow]Trace ID:[/yellow] {trace_id}\\n\")\n",
    "\n",
    "    # Establish a connection to the local Temporal server\n",
    "    client = await Client.connect(\"localhost:7233\")\n",
    "    # Specify the worker task queue dedicated to this example\n",
    "    task_queue = \"multi-agent-queue\"\n",
    "\n",
    "    # Start a Temporal worker hosting the workflow and activities\n",
    "    async with Worker(\n",
    "        client,\n",
    "        task_queue=task_queue,\n",
    "        workflows=[MultiAgentWorkflow],\n",
    "        activities=[triage_query, weather_agent, time_agent, get_weather, get_time],\n",
    "        workflow_runner=UnsandboxedWorkflowRunner(),\n",
    "        debug_mode=True,\n",
    "    ):\n",
    "        # Seed the example with a combined weather and time question\n",
    "        query = \"What's the weather like in London and what time is it there?\"\n",
    "        # Derive a workflow ID that embeds the trace reference\n",
    "        workflow_id = f\"multi-agent-{trace_id}\"\n",
    "\n",
    "        # Display the query being routed through the workflow\n",
    "        console.print(f\"[yellow]Query:[/yellow] {query}\\n\")\n",
    "\n",
    "        # Execute the workflow and wait for its final response\n",
    "        result = await client.execute_workflow(\n",
    "            MultiAgentWorkflow.run,\n",
    "            args=[query, trace_id],\n",
    "            id=workflow_id,\n",
    "            task_queue=task_queue,\n",
    "        )\n",
    "\n",
    "    # Show the natural language outcome returned by the agent chain\n",
    "    console.print(f\"\\n[bold green]âœ… Final Response:[/bold green]\\n{result}\\n\")\n",
    "    # Point operators to the Temporal UI entry for deeper inspection\n",
    "    console.print(\n",
    "        f\"[yellow]View in Temporal UI:[/yellow] \"\n",
    "        f\"http://localhost:8233/namespaces/default/workflows/{workflow_id}\"\n",
    "    )\n",
    "    # Echo the trace ID again for quick copy/paste access\n",
    "    console.print(f\"[yellow]Trace ID:[/yellow] {trace_id}\\n\")\n",
    "\n",
    "# Detect whether the notebook already has an active event loop\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "except RuntimeError:\n",
    "    # No loop means we can safely drive execution with asyncio.run\n",
    "    asyncio.run(run_solution())\n",
    "else:\n",
    "    # Apply nest_asyncio so we can reuse the existing notebook loop\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    # Schedule the solution coroutine on the current loop\n",
    "    task = loop.create_task(run_solution())\n",
    "    # Ensure the scheduled task completes before moving on\n",
    "    await task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d07c6",
   "metadata": {},
   "source": [
    "## Source Code\n",
    "\n",
    "The workflow orchestration and activity implementations are defined above in the first code cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
